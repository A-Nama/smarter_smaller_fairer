{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4601549,"sourceType":"datasetVersion","datasetId":2680265}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers torch accelerate bitsandbytes pandas tqdm sentencepiece --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-25T15:39:20.301273Z","iopub.execute_input":"2025-10-25T15:39:20.301564Z","iopub.status.idle":"2025-10-25T15:39:24.103337Z","shell.execute_reply.started":"2025-10-25T15:39:20.301545Z","shell.execute_reply":"2025-10-25T15:39:24.102429Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, LlamaTokenizer\nimport pandas as pd\nfrom tqdm import tqdm\nimport time\nimport numpy as np\nimport warnings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T15:42:15.950084Z","iopub.execute_input":"2025-10-25T15:42:15.950760Z","iopub.status.idle":"2025-10-25T15:42:15.954597Z","shell.execute_reply.started":"2025-10-25T15:42:15.950738Z","shell.execute_reply":"2025-10-25T15:42:15.953737Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# === 2. HIDE THE TOKENIZER WARNING ===\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nwarnings.filterwarnings(\"ignore\")\nprint(\"Tokenizer parallelism warning suppressed.\")\n\n# === 3. CONFIGURE AND LOAD THE MODEL ===\nmodel_id = \"apple/OpenELM-1_1B-Instruct\"\nmodel = None\ntokenizer = None\n\ntry:\n    # Config for 4-bit quantization\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16\n    )\n    \n    # --- LINE 2 TO CHANGE ---\n    # Instead of AutoTokenizer, we explicitly use LlamaTokenizer\n    tokenizer = LlamaTokenizer.from_pretrained(model_id)\n    \n    print(f\"Loading model: {model_id}...\")\n    # The model *still* needs trust_remote_code=True for its custom architecture\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        trust_remote_code=True\n    )\n    print(\"Model loaded successfully on GPU.\")\n\nexcept Exception as e:\n    print(f\"Error loading model: {e}\")\n    print(\"This might be a Kaggle GPU memory issue or a network problem. Try restarting the session.\")\n\n\n# === 4. DEFINE THE PERPLEXITY FUNCTION ===\ndef get_perplexity(text, model, tokenizer):\n    if not text or not isinstance(text, str):\n        return float('inf')\n    try:\n        inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n        input_ids = inputs.input_ids\n        \n        # Add a padding token if the tokenizer doesn't have one\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n            \n        with torch.no_grad():\n            outputs = model(input_ids, labels=input_ids)\n        mean_nll = outputs.loss\n        perplexity = torch.exp(mean_nll)\n        return perplexity.item()\n    except Exception as e:\n         return float('inf')\n\nprint(\"\\n--- Setup Complete ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T15:43:00.873703Z","iopub.execute_input":"2025-10-25T15:43:00.874368Z","iopub.status.idle":"2025-10-25T15:43:01.542790Z","shell.execute_reply.started":"2025-10-25T15:43:00.874336Z","shell.execute_reply":"2025-10-25T15:43:01.542065Z"}},"outputs":[{"name":"stdout","text":"Tokenizer parallelism warning suppressed.\n","output_type":"stream"},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n","output_type":"stream"},{"name":"stdout","text":"Error loading model: not a string\nThis might be a Kaggle GPU memory issue or a network problem. Try restarting the session.\n\n--- Setup Complete ---\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# === 5. LOAD DATA AND RUN AUDIT ===\n# Check if the model from Cell 1 was loaded successfully\nif 'model' in locals() and model is not None:\n    \n    # --- Load and Filter Dataset ---\n    print(\"Loading CrowS-Pairs dataset using pandas...\")\n    # This path comes from adding the dataset via \"+ Add Input\"\n    file_path = \"/kaggle/input/a-dataset-for-measuring-social-biases-in-mlms/crows_pairs_anonymized.csv\"\n    \n    try:\n        df = pd.read_csv(file_path)\n        print(f\"✅ Loaded {len(df)} records from CSV.\")\n        \n        if 'bias_type' not in df.columns:\n            print(\"Error: Column 'bias_type' not found in CSV.\")\n            gender_pairs_df = None\n        else:\n            gender_pairs_df = df[df['bias_type'] == 'gender'].copy()\n            print(f\"✅ Filtered {len(gender_pairs_df)} 'gender' pairs.\")\n            if len(gender_pairs_df) == 0:\n                print(\"Stopping execution: No gender pairs found after filtering.\")\n                gender_pairs_df = None\n                \n    except FileNotFoundError:\n        print(f\"Error: CSV file not found at {file_path}\")\n        print(\"Please use the '+ Add Input' button in the Kaggle sidebar to add the dataset.\")\n        gender_pairs_df = None\n    except Exception as e:\n        print(f\"An error occurred loading or filtering data: {e}\")\n        gender_pairs_df = None\n\n    # --- Run Audit (Only if data loading was successful) ---\n    if gender_pairs_df is not None:\n        bias_score_count = 0\n        processed_pairs = 0\n        total_pairs_to_process = len(gender_pairs_df)\n\n        print(f\"\\nRunning audit on {total_pairs_to_process} gender pairs for {model_id}...\")\n        start_time = time.time()\n\n        for index, pair in tqdm(gender_pairs_df.iterrows(), total=total_pairs_to_process):\n            try:\n                sent_more_stereo = pair['sent_more']\n                sent_less_anti_stereo = pair['sent_less']\n\n                if not isinstance(sent_more_stereo, str) or not isinstance(sent_less_anti_stereo, str) or not sent_more_stereo or not sent_less_anti_stereo:\n                    continue\n\n                ppl_stereo = get_perplexity(sent_more_stereo, model, tokenizer)\n                ppl_anti_stereo = get_perplexity(sent_less_anti_stereo, model, tokenizer)\n\n                if ppl_stereo == float('inf') or ppl_anti_stereo == float('inf'):\n                    continue\n\n                processed_pairs += 1\n\n                if ppl_stereo < ppl_anti_stereo:\n                    bias_score_count += 1\n\n            except Exception as e:\n                print(f\"Loop error processing index {index}: {e}\")\n                continue\n\n        end_time = time.time()\n        print(\"Audit complete!\")\n        run_duration = end_time - start_time\n\n        # --- Calculate and Print Final Score ---\n        final_bias_score = (bias_score_count / processed_pairs) * 100 if processed_pairs > 0 else 0\n\n        print(\"\\n\" + \"=\"*30)\n        print(f\"      FINAL RESULTS FOR: {model_id}\")\n        print(\"=\"*30)\n        print(f\"Total pairs attempted: {total_pairs_to_process}\")\n        print(f\"Pairs successfully processed: {processed_pairs}\")\n        print(f\"Pairs where stereotype was preferred: {bias_score_count}\")\n        print(f\"Audit duration: {run_duration:.2f} seconds ({run_duration/60:.2f} minutes)\")\n        print(f\"BIAS SCORE (Higher is worse): {final_bias_score:.2f}%\")\n        print(\"=\"*30)\n\nelse:\n    print(\"Model not loaded from Cell 1. Please run Cell 1 successfully before running Cell 2.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T15:31:01.682270Z","iopub.execute_input":"2025-10-25T15:31:01.682581Z","iopub.status.idle":"2025-10-25T15:33:28.807994Z","shell.execute_reply.started":"2025-10-25T15:31:01.682562Z","shell.execute_reply":"2025-10-25T15:33:28.807348Z"}},"outputs":[{"name":"stdout","text":"Loading CrowS-Pairs dataset using pandas...\n✅ Loaded 1508 records from CSV.\n✅ Filtered 262 'gender' pairs.\n\nRunning audit on 262 gender pairs for openbmb/MiniCPM-2B-sft-bf16...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 262/262 [02:27<00:00,  1.78it/s]","output_type":"stream"},{"name":"stdout","text":"Audit complete!\n\n==============================\n      FINAL RESULTS FOR: openbmb/MiniCPM-2B-sft-bf16\n==============================\nTotal pairs attempted: 262\nPairs successfully processed: 262\nPairs where stereotype was preferred: 162\nAudit duration: 147.09 seconds (2.45 minutes)\nBIAS SCORE (Higher is worse): 61.83%\n==============================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":12}]}